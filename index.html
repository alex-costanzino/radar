<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models">
  <meta name="keywords" content="InfoDiffusion, Diffusion Models, Representation Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.3/css/bulma.min.css">
    <style>
        .scrollable {
            max-width: 80%; /* Adjust the width as needed */
            overflow-x: auto;
            white-space: pre-wrap; /* This ensures that the text will wrap and not overflow */
            margin: 0 auto; /* Center the scrollable block */
        }
    </style>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://isjakewong.github.io/">Yingheng Wang</a>,</span>
            <span class="author-block">
              <a href="https://yair-schiff.github.io">Yair Schiff</a>,</span>
            <span class="author-block">
              <a href="https://skylion007.github.io">Aaron Gokaslan</a>,
            </span>
            <span class="author-block">
              <a href="https://vivo.weill.cornell.edu/display/cwid-wep4001">Weishen Pan</a>,
            </span>
            <br> <!-- Line break added here -->
            <span class="author-block">
              <a href="https://wcm-wanglab.github.io/">Fei Wang</a>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cornell.edu/~cdesa/">Chris De Sa</a>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cornell.edu/~kuleshov/">Volodymyr Kuleshov</a>
            </span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"></sup>Cornell University,</span>
            <span class="author-block"></sup>Cornell Tech, NY</span>
          </div>
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://proceedings.mlr.press/v202/wang23ah.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2306.08757"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <table style="width: 100%;">
        <tr>
          <td align="center" style="width: 50%;">
            <img src="./static/images/flowchart.drawio-1.png" width="99%" alt="Flowchart">
          </td>
          <td align="center" style="width: 50%;">
            <img src="./static/images/graphicalabstract.drawio_v2-1.png" width="99%" alt="Graphical Abstract">
          </td>
        </tr>
      </table>
      <h2 class="subtitle has-text-centered">
        (<em>Left</em>) Flow chart demonstrating auxiliary-variable diffusion model with mutual information and prior regularization. (<em>Right</em>) InfoDiffusion produces semantically meaningful latent space for a diffusion model. (<em>Top</em>) Smooth latent space. (<em>Bottom</em>) Disentangled, human-interpretable factors of variation.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While diffusion models excel at generating high-quality samples, their latent variables typically lack semantic meaning and are not suitable for representation learning.
            Here, we propose InfoDiffusion, an algorithm that augments diffusion models with low-dimensional latent variables that capture high-level factors of variation in the data.
            InfoDiffusion relies on a learning objective regularized with the mutual information between observed and hidden variables, which improves latent space quality and prevents the latents from being ignored by expressive diffusion-based decoders.
            Empirically, we find that InfoDiffusion learns disentangled and human-interpretable latent representations that are competitive with state-of-the-art generative and contrastive methods, while retaining the high sample quality of diffusion models. 
            Our method enables manipulating the attributes of generated images and has the potential to assist tasks that require exploring a learned latent space to generate quality samples, e.g., generative design.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

</section>
<div class="container is-max-desktop">
  <!-- Background -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Background</h2>
      <div class="content has-text-justified">
        <p>
          A diffusion model describes how data \(\mathbf{x}_0\) sampled from the data distribution transforms gradually into random Gaussian noise \(\mathbf{x}_T\) through a series of latent variables \(\mathbf{x}_{1:T}\). This process is defined by a latent variable distribution \(p(\mathbf{x}_{0:T})\), which can be factorized as a Markov chain:
        </p>
        <div style="text-align: center;">
          \( p(\mathbf{x}_{0:T}) = p(\mathbf{x}_T) \prod_{t=0}^{T-1} p_{\theta}(\mathbf{x}_t | \mathbf{x}_{t+1}) \)
        </div>
        <p>
          The noising process \(q\) starts with clean data \(\mathbf{x}_0\) from the data distribution \(q(\mathbf{x}_0)\) and gradually corrupts it through a sequence of variables \(\mathbf{x}_{1:T}\) in a Markov chain:
        </p>
        <div style="text-align: center;">
          \( q(\mathbf{x}_{1:T} | \mathbf{x}_0) = \prod_{t=1}^{T} q(\mathbf{x}_t | \mathbf{x}_{t-1}) \)
        </div>
        <p>
          Each step \(q(\mathbf{x}_t | \mathbf{x}_{t-1})\) is modeled as a Gaussian distribution with a schedule \(\alpha_t\). The marginal distribution of \(q\) is:
        </p>
        <div style="text-align: center;">
          \( q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, \sqrt{1 - \bar{\alpha}_t} \mathbf{I}) \)
        </div>
        <p>
          where \(\bar{\alpha}_t\) is the cumulative product of the schedule parameters \(\alpha_t\).
        </p>
        <p>
          To train \(p\), we maximize the evidence lower bound (ELBO) using variational inference:
        </p>
        <div style="text-align: center;">
          \( \log p(\mathbf{x}_0) \geq \mathbb{E}_{q(\mathbf{x}_1 | \mathbf{x}_0)} [ \log p_{\theta}(\mathbf{x}_0 | \mathbf{x}_1) ] - \text{KL}(q(\mathbf{x}_T | \mathbf{x}_0) \| p(\mathbf{x}_T)) - \sum_{t=2}^{T} \mathbb{E}_{q(\mathbf{x}_t | \mathbf{x}_0)} [ \text{KL}(q(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0) \| p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_t)) ]. \)
        </div>
      </div>
    </div>
  </div>
  <!--/ Background -->
  
  <!-- Unsupervised Representation Learning -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-5">Unsupervised Representation Learning</h2>
      <div class="content has-text-justified">
        <p>
          A key goal of generative modeling is representation learning: extracting latent concepts from data without supervision. Generative models \(p(\mathbf{x}, \mathbf{z})\) typically use low-dimensional variables \(\mathbf{z}\) to represent latent concepts, inferred via posterior inference over \(p(\mathbf{z} | \mathbf{x})\).
          Variational Autoencoders (VAEs) follow this framework but do not produce state-of-the-art samples. Diffusion models, on the other hand, generate high-quality samples but lack an interpretable low-dimensional latent space, making them less suitable for representation learning.
        </p>
      </div>
    </div>
  </div>
  <!--/ Unsupervised Representation Learning -->
  
  <!-- Diffusion Models With Auxiliary Latents -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4">Diffusion Models With Auxiliary Latents</h2>
      <div class="content has-text-justified">
        <p>
          We introduce a diffusion model with a semantically meaningful latent space while maintaining high sample quality. Our approach includes three main steps:
        </p>
        <ol>
          <li>Define a diffusion model family with low-dimensional latent variables.</li>
          <li>Specify learning objectives for this model family.</li>
          <li>Introduce a regularizer based on mutual information to enhance the quality of latents.</li>
        </ol>
        <p>
          We define an auxiliary-variable diffusion model as:
        </p>
        <div style="text-align: center;">
          \( p(\mathbf{x}_{0:T}, \mathbf{z}) = p(\mathbf{x}_T) p(\mathbf{z}) \prod_{t=1}^{T} p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{z}) \)
        </div>
        <p>
          This model performs a reverse diffusion process \(p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{z})\) over \(\mathbf{x}_{0:T}\) conditioned on auxiliary latents \(\mathbf{z}\), which are distributed according to a prior \(p(\mathbf{z})\).
        </p>
      </div>
    </div>
  </div>
  <!--/ Diffusion Models With Auxiliary Latents -->
  
  <!-- Auxiliary Latent Variables and Semantic Prior -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-5">Auxiliary Latent Variables and Semantic Prior</h2>
      <div class="content has-text-justified">
        <p>
          The auxiliary latents \(\mathbf{z}\) aim to encode high-level representations of \(\mathbf{x}_0\). These latents are not restricted in dimension and can be continuous or discrete, representing various factors of variation.
          The prior \(p(\mathbf{z})\) ensures a principled probabilistic model, allowing unconditional sampling of \(\mathbf{x}_0\) and encoding domain knowledge about \(\mathbf{z}\). For example, if the dataset has \(K\) distinct classes, we can set \(p(\mathbf{z})\) to be a mixture of \(K\) components.
        </p>
      </div>
    </div>
  </div>
  <!--/ Auxiliary Latent Variables and Semantic Prior -->
  
  <!-- Auxiliary-Variable Diffusion Decoder -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-5">Auxiliary-Variable Diffusion Decoder</h2>
      <div class="content has-text-justified">
        <p>
          The decoder \(p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{z})\) is conditioned on the auxiliary latents \(\mathbf{z}\). In a trained model, \(\mathbf{z}\) captures high-level concepts (e.g., age or skin color), while the sequence of \(\mathbf{x}_t\) progressively adds lower-level details (e.g., hair texture).
          Following previous work, we define the decoder as:
        </p>
        <div style="text-align: center;">
          \( p_{\theta}(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{z}) = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t - (1-\alpha_t) / \sqrt{1-\bar{\alpha}_t} \epsilon_{\theta}(\mathbf{x}_{t-1}, t, \mathbf{z})) \)
        </div>
      </div>
    </div>
  </div>
  <!--/ Auxiliary-Variable Diffusion Decoder -->

    <!-- InfoDiffusion: Regularizing Semantic Latents By Maximizing Mutual Information -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">InfoDiffusion: Regularizing Semantic Latents By Maximizing Mutual Information</h2>
        <div class="content has-text-justified">
          <p>
            Diffusion models with auxiliary latents face two main risks. First, an expressive decoder \(p_\theta(\mathbf{x}_{t-1}\mid\mathbf{x}_t, \mathbf{z})\) may ignore low-dimensional latents \(\mathbf{z}\) and generate \(\mathbf{x}_{t-1}\) unconditionally. Second, the approximate posterior \(q_\phi(\mathbf{z}\mid\mathbf{x}_0)\) may fail to match the prior \(p(\mathbf{z})\) due to weak prior regularization relative to the reconstruction term. This degrades the quality of both ancestral sampling and latent representations.
          </p>
          
          <h3 class="title is-5">Regularizing Auxiliary-Variable Diffusion</h3>
          <p>
            To address these issues, we propose two regularization terms: a mutual information term and a prior regularizer. We refer to this approach as InfoDiffusion.
          </p>
          
          <h4 class="title is-6">Mutual Information Regularization</h4>
          <p>
            To ensure the model does not ignore the latents \(\mathbf{z}\), we augment the learning objective with a mutual information term between \(\mathbf{x}_0\) and \(\mathbf{z}\) under \(q_\phi(\mathbf{x}_0, \mathbf{z})\), the joint distribution over observed data \(\mathbf{x}_0\) and latent variables \(\mathbf{z}\). Formally, the mutual information regularizer is defined as:
          </p>
          <div style="text-align: center;">
            \(\mathrm{MI}_{\mathbf{x}_0, \mathbf{z}} = \mathbb{E}_{q_\phi(\mathbf{x}_0, \mathbf{z})}\left[ \log \frac{q_\phi(\mathbf{x}_0, \mathbf{z})}{q(\mathbf{x}_0)q_\phi(\mathbf{z})} \right]\)
          </div>
          <p>
            Here, \(q_\phi(\mathbf{z})\) is the marginal approximate posterior distribution, defined as the marginal of the product \(q_\phi(\mathbf{z}\mid\mathbf{x}_0)q(\mathbf{x}_0)\). Maximizing mutual information encourages the model to generate \(\mathbf{x}_0\) from which \(\mathbf{z}\) can be predicted.
          </p>
          
          <h4 class="title is-6">Prior Regularization</h4>
          <p>
            To prevent a degenerate approximate posterior, we regularize the encoded samples \(\mathbf{z}\) to resemble the prior \(p(\mathbf{z})\). The prior regularizer is defined as:
          </p>
          <div style="text-align: center;">
            \(\mathcal{R} = \text{D}(q_\phi(\mathbf{z})\|p(\mathbf{z}))\)
          </div>
          <p>
            where \(\text{D}\) is any strict divergence.
          </p>
          
          <h3 class="title is-5">A Tractable Objective for InfoDiffusion</h3>
          <p>
            We train InfoDiffusion by maximizing a regularized ELBO objective:
          </p>
          <div style="text-align: center;">
            \(\mathbb{E}_{q(\mathbf{x}_0)}[\mathcal{L}_D(\mathbf{x}_0)] + \zeta \cdot \mathrm{MI}_{\mathbf{x}_0, \mathbf{z}} - \beta \cdot \mathcal{R}\)
          </div>
          <p>
            where \(\mathcal{L}_D(\mathbf{x}_0)\) is from the denoising objective, and \(\zeta, \beta > 0\) are scalars controlling the regularization strength. We can rewrite this objective into a tractable form:
          </p>
            <div style="text-align: center;">
            \(\mathcal{L}_I = \mathbb{E}_{q(\mathbf{x}_0, \mathbf{x}_1)}\left[\mathbb{E}_{q_{\mathbf{z}}}\left[\log p_{\theta}(\mathbf{x}_0|\mathbf{x}_1, \mathbf{z})\right]\right] - \mathbb{E}_{q(\mathbf{x}_0)}[\mathrm{KL}(q(\mathbf{x}_T | \mathbf{x}_0) \| p(\mathbf{x}_T))] \)
            \(- \sum_{t=2}^{T} \mathbb{E}_{q(\mathbf{x}_0, \mathbf{x}_t)}\left[\mathbb{E}_{q_{\mathbf{z}}} \left[\mathrm{KL}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) \| p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{z}))\right]\right]\)
            \(- (1-\zeta) \mathbb{E}_{q(\mathbf{x}_0)} [\mathrm{KL}(q_\phi(\mathbf{z}\mid\mathbf{x}_0)\|p(\mathbf{z}))] - (\lambda + \zeta-1) \mathrm{KL}(q_\phi(\mathbf{z}) \| p(\mathbf{z}))\)
          </div>
          where We now state that \( \text{KL}(q_\phi(\mathbf{z}) || p(\mathbf{z})) \) can be replaced with any strict divergence \( \text{D}(q_\phi(\mathbf{z}) || p(\mathbf{z})) \) without any modification (see our paper for the full derivation).
          
          <h3 class="title is-5">Comparing InfoDiffusion to Existing Models</h3>
          <p>
            The InfoDiffusion algorithm generalizes several existing methods. When the decoder performs one step of diffusion (\(T=1\)), it is equivalent to <a href="https://arxiv.org/abs/1706.02262">InfoVAE</a> Choosing \(\lambda=0\) recovers the <a href="https://openreview.net/forum?id=Sy2fzU9gl">\(\beta\)-VAE</a> model. When \(T=1\) and \(\text{D}\) is the Jensen-Shannon divergence, it recovers <a href="https://arxiv.org/abs/1511.05644">adversarial auto-encoders (AAEs)</a>. InfoDiffusion extends these models to diffusion decoders, similar to how DDPM extends VAEs. When \(\zeta=\lambda=0\), it recovers <a href="https://arxiv.org/abs/2111.15640">DiffAE</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ InfoDiffusion: Regularizing Semantic Latents By Maximizing Mutual Information -->
  
    <!-- Citation -->
    <div class="columns is-centered">
        <div class="column is-full-width">
            <h2 class="title is-4">BibTeX</h2>
            <div class="content has-text-justified">
                <pre class="scrollable"><code>@inproceedings{wang2023infodiffusion,
      title={Infodiffusion: Representation learning using information maximizing diffusion models},
      author={Wang, Yingheng and Schiff, Yair and Gokaslan, Aaron and Pan, Weishen and Wang, Fei and De Sa, Christopher and Kuleshov, Volodymyr},
      booktitle={International Conference on Machine Learning},
      pages={36336--36354},
      year={2023},
      organization={PMLR}
    }</code></pre>
            </div>
        </div>
    </div>
    <!--/ Citation -->


  </div>
 </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This webpage was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template">Academic Project Page Template</a> which was adopted from the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page. You are free to borrow the of this website, we just ask that you link back to this page in the footer.
          </p>
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            Please remember to remove the analytics code included in the header of the website which you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
