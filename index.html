<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models">
  <meta name="keywords" content="InfoDiffusion, Diffusion Models, Representation Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">InfoDiffusion: Representation Learning Using Information Maximizing Diffusion Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://isjakewong.github.io/">Yingheng Wang</a>,</span>
            <span class="author-block">
              <a href="https://yair-schiff.github.io">Yair Schiff</a>,</span>
            <span class="author-block">
              <a href="https://skylion007.github.io">Aaron Gokaslan</a>,
            </span>
            <span class="author-block">
              <a href="https://vivo.weill.cornell.edu/display/cwid-wep4001">Weishen Pan</a>,
            </span>
            <br> <!-- Line break added here -->
            <span class="author-block">
              <a href="https://wcm-wanglab.github.io/">Fei Wang</a>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cornell.edu/~cdesa/">Chris De Sa</a>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.cornell.edu/~kuleshov/">Volodymyr Kuleshov</a>
            </span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"></sup>Cornell University,</span>
            <span class="author-block"></sup>Cornell Tech, NY</span>
          </div>
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://proceedings.mlr.press/v202/wang23ah.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2306.08757"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <table style="width: 100%;">
        <tr>
          <td align="center" style="width: 50%;">
            <img src="./static/images/flowchart.drawio-1.png" width="99%" alt="Flowchart">
          </td>
          <td align="center" style="width: 50%;">
            <img src="./static/images/graphicalabstract.drawio_v2-1.png" width="99%" alt="Graphical Abstract">
          </td>
        </tr>
      </table>
      <h2 class="subtitle has-text-centered">
        (<em>Left</em>) Flow chart demonstrating auxiliary-variable diffusion model with mutual information and prior regularization. (<em>Right</em>) InfoDiffusion produces semantically meaningful latent space for a diffusion model. (<em>Top</em>) Smooth latent space. (<em>Bottom</em>) Disentangled, human-interpretable factors of variation.
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While diffusion models excel at generating high-quality samples, their latent variables typically lack semantic meaning and are not suitable for representation learning.
            Here, we propose InfoDiffusion, an algorithm that augments diffusion models with low-dimensional latent variables that capture high-level factors of variation in the data. % (e.g., high-level image attributes like hair color, eye color, etc.)
            InfoDiffusion relies on a learning objective regularized with the mutual information between observed and hidden variables, which improves latent space quality and prevents the latents from being ignored by expressive diffusion-based decoders.
            Empirically, we find that InfoDiffusion learns disentangled and human-interpretable latent representations that are competitive with state-of-the-art generative and contrastive methods, while retaining the high sample quality of diffusion models. 
            Our method enables manipulating the attributes of generated images and has the potential to assist tasks that require exploring a learned latent space to generate quality samples.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

</section>
  <div class="container is-max-desktop">
    <!-- Background -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Background</h2>
        <div class="content has-text-justified">
          <p>
            A diffusion model describes how data <em>(x<sub>0</sub>)</em> sampled from the data distribution transforms gradually into random Gaussian noise <em>(x<sub>T</sub>)</em> through a series of latent variables <em>(x<sub>1:T</sub>)</em>. This process is defined by a latent variable distribution <em>(p(x<sub>0:T</sub>))</em>, which can be factorized as a Markov chain:
          </p>
          <pre>
p(x<sub>0:T</sub>) = p(x<sub>T</sub>) ‚àè<sub>t=0</sub><sup>T-1</sup> p<sub>Œ∏</sub>(x<sub>t</sub> | x<sub>t+1</sub>).
          </pre>
          <p>
            This Markov chain maps noise <em>(x<sub>T</sub>)</em> back into data <em>(x<sub>0</sub>)</em> by reversing a noising process <em>(q)</em>. Here, we use a learned denoising distribution <em>(p<sub>Œ∏</sub>)</em> parameterized by a neural network with parameters <em>Œ∏</em>.
          </p>
          <p>
            The noising process <em>(q)</em> starts with clean data <em>(x<sub>0</sub>)</em> from the data distribution <em>(q(x<sub>0</sub>))</em> and gradually corrupts it through a sequence of variables <em>(x<sub>1:T</sub>)</em> in a Markov chain:
          </p>
          <pre>
q(x<sub>1:T</sub> | x<sub>0</sub>) = ‚àè<sub>t=1</sub><sup>T</sup> q(x<sub>t</sub> | x<sub>t-1</sub>).
          </pre>
          <p>
            Each step <em>(q(x<sub>t</sub> | x<sub>t-1</sub>))</em> is modeled as a Gaussian distribution with a schedule <em>(Œ±<sub>t</sub>)</em>. The marginal distribution of <em>(q)</em> is:
          </p>
          <pre>
q(x<sub>t</sub> | x<sub>0</sub>) = ùí©(x<sub>t</sub>; ‚àö<bar>Œ±<sub>t</sub>x<sub>0</sub>, ‚àö(1-<bar>Œ±<sub>t</sub>)ùë∞),
          </pre>
          <p>
            where <em><bar>Œ±<sub>t</sub></em> is the cumulative product of the schedule parameters <em>(Œ±<sub>t</sub>)</em>.
          </p>
          <p>
            To train <em>(p)</em>, we maximize the evidence lower bound (ELBO) using variational inference:
          </p>
          <pre>
log p(x<sub>0</sub>) ‚â• ùîº<sub>q(x<sub>1</sub> | x<sub>0</sub>)</sub>[log p<sub>Œ∏</sub>(x<sub>0</sub> | x<sub>1</sub>)]
- KL(q(x<sub>T</sub> | x<sub>0</sub>) || p(x<sub>T</sub>))
- ‚àë<sub>t=2</sub><sup>T</sup> ùîº<sub>q(x<sub>t</sub> | x<sub>0</sub>)</sub>[KL(q(x<sub>t-1</sub> | x<sub>t</sub>, x<sub>0</sub>) || p<sub>Œ∏</sub>(x<sub>t-1</sub> | x<sub>t</sub>))],
          </pre>
          <p>
            where <em>KL</em> denotes the Kullback‚ÄìLeibler divergence.
          </p>
        </div>
      </div>
    </div>
    <!--/ Background -->
    
    <!-- Unsupervised Representation Learning -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Unsupervised Representation Learning</h2>
        <div class="content has-text-justified">
          <p>
            A key goal of generative modeling is representation learning: extracting latent concepts from data without supervision. Generative models <em>(p(x, z))</em> typically use low-dimensional variables <em>(z)</em> to represent latent concepts, inferred via posterior inference over <em>(p(z | x))</em>.
          </p>
          <p>
            Variational Autoencoders (VAEs) follow this framework but do not produce state-of-the-art samples. Diffusion models, on the other hand, generate high-quality samples but lack an interpretable low-dimensional latent space, making them less suitable for representation learning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Unsupervised Representation Learning -->
    
    <!-- Diffusion Models With Auxiliary Latents -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Diffusion Models With Auxiliary Latents</h2>
        <div class="content has-text-justified">
          <p>
            This paper introduces a diffusion model with a semantically meaningful latent space while maintaining high sample quality. Our approach includes three main steps:
          </p>
          <ol>
            <li>Define a diffusion model family with low-dimensional latent variables.</li>
            <li>Specify learning objectives for this model family.</li>
            <li>Introduce a regularizer based on mutual information to enhance the quality of latents.</li>
          </ol>
          <p>
            We define an auxiliary-variable diffusion model as:
          </p>
          <pre>
p(x<sub>0:T</sub>, z) = p(x<sub>T</sub>) p(z) ‚àè<sub>t=1</sub><sup>T</sup> p<sub>Œ∏</sub>(x<sub>t-1</sub> | x<sub>t</sub>, z).
          </pre>
          <p>
            This model performs a reverse diffusion process <em>(p<sub>Œ∏</sub>(x<sub>t-1</sub> | x<sub>t</sub>, z))</em> over <em>(x<sub>0:T</sub>)</em> conditioned on auxiliary latents <em>(z)</em>, which are distributed according to a prior <em>(p(z))</em>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Diffusion Models With Auxiliary Latents -->
    
    <!-- Auxiliary Latent Variables and Semantic Prior -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Auxiliary Latent Variables and Semantic Prior</h2>
        <div class="content has-text-justified">
          <p>
            The auxiliary latents <em>(z)</em> aim to encode high-level representations of <em>(x<sub>0</sub>)</em>. These latents are not restricted in dimension and can be continuous or discrete, representing various factors of variation.
          </p>
          <p>
            The prior <em>(p(z))</em> ensures a principled probabilistic model, allowing unconditional sampling of <em>(x<sub>0</sub>)</em> and encoding domain knowledge about <em>(z)</em>. For example, if the dataset has <em>K</em> distinct classes, we can set <em>p(z)</em> to be a mixture of <em>K</em> components.
          </p>
        </div>
      </div>
    </div>
    <!--/ Auxiliary Latent Variables and Semantic Prior -->
    
    <!-- Auxiliary-Variable Diffusion Decoder -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Auxiliary-Variable Diffusion Decoder</h2>
        <div class="content has-text-justified">
          <p>
            The decoder <em>(p<sub>Œ∏</sub>(x<sub>t-1</sub> | x<sub>t</sub>, z))</em> is conditioned on the auxiliary latents <em>(z)</em>. In a trained model, <em>(z)</em> captures high-level concepts (e.g., age or skin color), while the sequence of <em>(x<sub>t</sub>)</em> progressively adds lower-level details (e.g., hair texture).
          </p>
          <p>
            Following previous work, we define the decoder as:
          </p>
          <pre>
p<sub>Œ∏</sub>(x<sub>t-1</sub> | x<sub>t</sub>, z) = 1/‚àö<Œ±<sub>t</sub>(x<sub>t</sub> - (1-Œ±<sub>t</sub>) / ‚àö(1-<bar>Œ±<sub>t</sub>)Œµ<sub>Œ∏</sub>(x<sub>t-1</sub>, t, z)),
          </pre>
          <p>
            with a noise prediction network <em>(Œµ<sub>Œ∏</sub>(x<sub>t-1</sub>, t, z))</em> parameterized by a U-Net.
          </p>
        </div>
      </div>
    </div>
    <!--/ Auxiliary-Variable Diffusion Decoder -->

    <!-- Citation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Citation</h2>
        <div class="content has-text-justified">
          <pre><code>@inproceedings{wang2023infodiffusion,
    title={Infodiffusion: Representation learning using information maximizing diffusion models},
    author={Wang, Yingheng and Schiff, Yair and Gokaslan, Aaron and Pan, Weishen and Wang, Fei and De Sa, Christopher and Kuleshov, Volodymyr},
    booktitle={International Conference on Machine Learning},
    pages={36336--36354},
    year={2023},
    organization={PMLR}
}</code></pre>
        </div>
      </div>
    </div>
    <!--/ Citation. -->

  </div>
 </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
